{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83a1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/damianzeller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/damianzeller/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from currency_converter import CurrencyConverter\n",
    "import re \n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8ba7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/damianzeller/anaconda3/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/damianzeller/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/damianzeller/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/damianzeller/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/damianzeller/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85256a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/damianzeller/Desktop/HS23/ADM/dataframe_HW3.csv\")\n",
    "df['description'] = df['description'].str.replace('About the course', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5996f70",
   "metadata": {},
   "source": [
    "2.0.0 Processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18946019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for general processing\n",
    "def processing(x):\n",
    "    #make sure that input is a string and case insensitive\n",
    "    words = nltk.word_tokenize(str(x).lower())\n",
    "    #stopwords, punctuation and nonnumerical\n",
    "    lst_stopwords = stopwords.words('english')\n",
    "    puncto = string.punctuation\n",
    "    text_preproc= [word for word in words if not word in lst_stopwords and word.isalpha() and word not in puncto]\n",
    "    #return the processed text as string\n",
    "    return ' '.join(text_preproc)\n",
    "\n",
    "#Delete the rows with no description\n",
    "df = df[df.description != '']\n",
    "\n",
    "#Apply the function to the description column and store result in desc_preproc\n",
    "df['desc_preproc'] = df['description'].apply(processing)\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmo_clean'] = df.desc_preproc.apply(lambda row: [stemmer.stem(word) for word in row.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce889fc",
   "metadata": {},
   "source": [
    "2.0.1 Preprocessing the fees column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc2821",
   "metadata": {},
   "source": [
    "First we want to extract the currency, we will do this by applying a function and storing the result in a new column called currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746692d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_currency(currency):\n",
    "    # Exception handling: Check if input is a string or if there is \n",
    "    if isinstance(currency, str):\n",
    "        # Find all currency symbols in the string\n",
    "        symbols = re.findall(r'[£$€¥¢]', currency)\n",
    "        # Return the first symbol found, or None if no symbols are found,also handels empty strings\n",
    "        return symbols[0] if symbols else None\n",
    "    else:\n",
    "        # Handle non-string inputs\n",
    "        return None\n",
    "#Applying the function to all rows in the dataframe and store the result in new column 'currency' \n",
    "df['currency'] = df['fees'].apply(extract_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eda5ef",
   "metadata": {},
   "source": [
    "In a second step we extract the highest fee and store the result in a new column highest_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcb51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_highest_fee(fee):\n",
    "    if isinstance(fee, list):\n",
    "        # Exception handling: If the fee is a list, convert each item to a string and apply the function recursively\n",
    "        return max(extract_highest_fee(item) for item in fee) if fee else None\n",
    "    elif isinstance(fee, str):\n",
    "        # If the fee is a string, we just find all numbers and select the highest\n",
    "        numbers = re.findall(r'\\d+', fee)\n",
    "        numbers = [int(num) for num in numbers]\n",
    "        return max(numbers) if numbers else None\n",
    "    elif isinstance(fee, float):\n",
    "        # If the fee is a float, we don't have to do anything\n",
    "        return fee\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(fee)}\")\n",
    "\n",
    "df['highest_fee'] = df['fees'].apply(extract_highest_fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2428c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "£    1102\n",
      "€     283\n",
      "$      32\n",
      "Name: currency, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "which_currency = df['currency'].value_counts()\n",
    "print(which_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1276933",
   "metadata": {},
   "source": [
    "There are only three different currencies, we decide to convert them to euros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69bc8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eur(row):\n",
    "    #Write a currency mapping in order to make them convertible\n",
    "    currency_mapping = {\n",
    "    '$': 'USD',\n",
    "    '€': 'EUR',\n",
    "    '£': 'GBP',}\n",
    "    #Handling the case, that there was no currency found before\n",
    "    if pd.isnull(row['currency']):\n",
    "        return None\n",
    "    #Fees that we actually have to convert: Either Dollar or British Pound\n",
    "    elif row['currency'] != 'EUR':\n",
    "        cc = CurrencyConverter()\n",
    "        currency_code = currency_mapping.get(row['currency'])\n",
    "        if currency_code is not None:\n",
    "            currency_value = cc.convert(1, currency_code, 'EUR') * row['highest_fee']\n",
    "            return currency_value\n",
    "        #Exception handling: \n",
    "        else:\n",
    "            raise ValueError(f\"{row['currency']} is not a supported currency\")\n",
    "    #Currency is already Euros, we don't have to do anything        \n",
    "    else:\n",
    "        return row['highest_fee']\n",
    "\n",
    "#Apply the function to all rows in the dataframe and store it in the new column 'fees_in_eur'\n",
    "df['fees_in_eur'] = df.apply(convert_to_eur, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df19394",
   "metadata": {},
   "source": [
    "Now we have everything we need in the column 'fees_in_eur'. We delete the old 'fees' column and rename the column 'fees_in_eur' to 'fees'. Further we delete the columns 'currency' and 'highest_fee' as we don't need them anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde8ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the old 'fees' column\n",
    "df.drop(['fees'], axis=1, inplace=True)\n",
    "\n",
    "#Rename the 'fees_in_eur' column\n",
    "df.rename(columns={'fees_in_eur': 'fees'}, inplace=True)\n",
    "\n",
    "#Delete the 'highest_fee' and the 'currency' column\n",
    "df.drop(['highest_fee', 'currency'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d74864",
   "metadata": {},
   "source": [
    "We print the first 100 rows of the dataframe and our results seem reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e827ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d4409",
   "metadata": {},
   "source": [
    "2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4746f",
   "metadata": {},
   "source": [
    "2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4a781",
   "metadata": {},
   "source": [
    "First we build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "#Put the elements of 'stemmo_clean' column into the vocabulary\n",
    "df.stemmo_clean.apply(lambda row: [vocabulary.add(word) for word in row])\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c5cab",
   "metadata": {},
   "source": [
    "Now we assign an ID to each word in the vocabulary. We do this by creating a dictionnary called index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a dictionnary\n",
    "index = {}\n",
    "\n",
    "#We make a list out of the vocabulary in order to loop over it\n",
    "vocabulary = list(vocabulary) \n",
    "\n",
    "#Initialize first ID\n",
    "unique_id = 1\n",
    "\n",
    "#Loop over the vocabulary\n",
    "for word in vocabulary:\n",
    "    index[unique_id] = word\n",
    "    unique_id+=1\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6e9b6",
   "metadata": {},
   "source": [
    "We create the inverted index. It's a dictionnary called 'inverted_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4146085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "#Function to return True or False if the word is there\n",
    "def isthere(x, value):\n",
    "    if value in x:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Loop over the 'index dictionnary\n",
    "for key, value in index.items():\n",
    "    #Initialize a list to store all the row numbers where to word is present\n",
    "    listo = []\n",
    "    #Loop over the dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        #Function is called and adds every row where the word is present to the list\n",
    "        if isthere(row['stemmo_clean'], value):\n",
    "            listo.append(i)\n",
    "    #When we looped over all rows the list with the row numbers, the list is being assigned as value ro to correspoinding ID of the word\n",
    "    inverted_index[key] = listo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ad5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd38f8",
   "metadata": {},
   "source": [
    "We create our first search engine. We do this by writing a fucntion that is called search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fb30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    #We split the input into single words and initialize a list\n",
    "    query_words = query.split()\n",
    "    document_lists = []\n",
    "    #Loop over the individual words\n",
    "    for word in query_words:\n",
    "        #As the words in our documents are preprocessed and stemmed we have to the same with our query\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        #We check if the word shows up in our documents and was assigned an ID\n",
    "        if stemmed_word in index.values():\n",
    "            #Loop over our 'index' dictionnary\n",
    "            for key, value in index.items():\n",
    "                if value == stemmed_word:\n",
    "                    #We add the the list of documents where the word showed up to the 'document_lists'\n",
    "                    document_lists.append(inverted_index[key])\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Unfortunately your search was not successful.\")\n",
    "    #If the search was successful we compute the intersection of the lists(as all the words must show up in the document)\n",
    "    if document_lists:\n",
    "        common_documents = set(document_lists[0]).intersection(*document_lists)\n",
    "        #Create the dataframe as requested in the task\n",
    "        new_df = df.loc[common_documents, ['courseName', 'universityName', 'description', 'url']].copy()\n",
    "        return new_df\n",
    "    #If there is no intersection between the lists of documents the search was not successfull\n",
    "    else:\n",
    "        print(f\"Unfortunately your search was not successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40fb19",
   "metadata": {},
   "source": [
    "Now we check if our search engine works with the query used as an example in the task. The results we get seem reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb438052",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('advanced knowledge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc770ab",
   "metadata": {},
   "source": [
    "2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee1685",
   "metadata": {},
   "source": [
    "First we calculate our second inverted index, we call it 'inverted_index_due'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbdea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform our 'stemmo_clean' column to a list of strings. Each string is a document\n",
    "documents = [' '.join(words) for words in df['stemmo_clean'].tolist()]\n",
    "\n",
    "#Process data to be able to calculate the TFIDF\n",
    "matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names \n",
    "transformed_words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize the dictionnary\n",
    "inverted_index_due = {}\n",
    "\n",
    "# Loop over all the words\n",
    "for index, transformed_word in enumerate(transformed_words):\n",
    "    #Create the dictionnary key\n",
    "    inverted_index_due[transformed_word] = []\n",
    "    #Loop over all the rows\n",
    "    for row in range(len(df)):  \n",
    "        #Create tfidf score\n",
    "        tfidf_score = matrix[row, index]\n",
    "        #Documents with a tfidf of are left out\n",
    "        if tfidf_score > 0:\n",
    "            #Assign the tuple as value\n",
    "            inverted_index_due[transformed_word].append((df.index[index], tfidf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3671d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Solve the about the course\n",
    "2. Proper preprocessing\n",
    "3. Clean dataframe: Make a string out of the list\n",
    "4. Preprocess inputqueries\n",
    "5. TSV file\n",
    "6. Save all the things that take long to compute in a file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e4a56",
   "metadata": {},
   "source": [
    "After we have created our inverted index, we create our second serach engine, we call it how_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d98a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_similar(query):\n",
    "    \n",
    "    #We preprocess our input quety\n",
    "    query= processing(query)\n",
    "    words = word_tokenize(query)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    stemmed_query = ' '.join(stemmed_words)\n",
    "    \n",
    "    # Make TFIDF representation of our query\n",
    "    query_tfidf = tfidf_vectorizer.transform([stemmed_query])\n",
    "    \n",
    "    # Calculate cosine similarities of the query with the documents\n",
    "    similarities = cosine_similarity(query_tfidf,matrix)\n",
    "    similarities = similarities.flatten()\n",
    "    \n",
    "    #We create the dataframe requested in the task\n",
    "    result_df= pd.DataFrame({\n",
    "    'courseName': df['courseName'],\n",
    "    'universityName': df['universityName'],\n",
    "    'description': df['description'],\n",
    "    'url': df['url'],\n",
    "    'similarity_score': similarities})\n",
    "    \n",
    "    #Sort, so that the most similar documents are shown first\n",
    "    result_df = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_similar('advanced knowledge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604d548",
   "metadata": {},
   "source": [
    "We test our search engine with some input and the results seem reasonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
