{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/laura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/laura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "import re\n",
    "from currency_converter import CurrencyConverter\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_url = \"https://www.findamasters.com/masters-degrees/msc-degrees/\"\n",
    "urls = \"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the urls of all the 400 pages\n",
    "def generate_urls(start_url, num_pages):\n",
    "   url_list = [start_url.replace(\"?PG=2\", f\"?PG={i}\") for i in range(1, num_pages+1)]\n",
    "   return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we change the first element on the list since the url is different from the rest of the urls\n",
    "url_list = generate_urls(urls, 400)\n",
    "url_list[0] = first_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the file masterdegree_url.txt\n",
    "with open(\"masterdegree_url.txt\", \"w\") as fl:\n",
    "  for x in url_list:\n",
    "      # we get the links of each master in each link\n",
    "      result_url = requests.get(x)\n",
    "      result_soup = BeautifulSoup(result_url.text, 'html.parser')\n",
    "      result_links = result_soup.find_all('a', {'class': 'courseLink'})\n",
    "      for item in result_links:\n",
    "        fl.write(f\"www.findamasters.com{item.get('href')}\\n\")\n",
    "      # We use the time sleep to avoid being blocked by the website since we are doing a lot of requests simultanelly\n",
    "      time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this to obtain a list of the urls of each master to parse it through the next function\n",
    "with open('masterdegree_url.txt', 'r') as f:\n",
    "   urls = [line.strip() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some counters to order the list 1-15 since each page has 15 masters\n",
    "r = 0\n",
    "t = 15\n",
    "for x in tqdm(range(400)):\n",
    "    # Create directory for the page we are on\n",
    "    os.makedirs(\"pagina\" + str(x))\n",
    "    urls_15 = urls[r:t]\n",
    "    num = 1\n",
    "    # In this for loop we obtain all 15 links inside the page \n",
    "    for url in urls_15:\n",
    "        response = requests.get(\"https://\" + str(url))\n",
    "        # Parse the HTML content\n",
    "        soup = response.text\n",
    "        # Save the HTML content to a file\n",
    "        with open((f'pagina{x}/master{num}.html'), 'w') as f:\n",
    "            f.write(soup)\n",
    "        num += 1\n",
    "        time.sleep(1)\n",
    "    r = t \n",
    "    t = t +15   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folders saved in 1.2\n",
    "path = '/home/laura/Data_Science/ADM'\n",
    "\n",
    "# Creation of list to store dictionnary for each course\n",
    "courses = []\n",
    "\n",
    "#Iterate over the folders\n",
    "for folder in os.listdir(path):\n",
    "  folder_path = os.path.join(path, folder)\n",
    "  \n",
    "    #Check if it's a directory\n",
    "  if os.path.isdir(folder_path):      \n",
    "    #Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "      if filename.endswith('.html'):\n",
    "        file_path = os.path.join(folder_path, filename)    \n",
    "          # Open the file\n",
    "        with open(file_path, 'r') as f:\n",
    "          contents = f.read()\n",
    "          \n",
    "          # Parse the HTML\n",
    "          soup = BeautifulSoup(contents, 'html.parser')\n",
    "          \n",
    "          course_info = {} \n",
    "          \n",
    "          #Course name\n",
    "          try:\n",
    "            courseName = soup.find_all('h1', {'class':'course-header__course-title'})\n",
    "            #course_info = {} # Initialize the dictionary\n",
    "            course_info['courseName']= courseName[0].text.strip()\n",
    "          except:\n",
    "            course_info['courseName']= ''\n",
    "          \n",
    "          #University name\n",
    "          try:\n",
    "            page_links = soup.find_all('a', {'class': 'course-header__institution'})\n",
    "            #course_info = {} # Initialize the dictionary\n",
    "            course_info['universityName'] = page_links[0].contents[0]\n",
    "          except:\n",
    "            course_info['universityName'] = ''\n",
    "\n",
    "          #Faculty name\n",
    "          try:\n",
    "            faculty_links = soup.find_all('a', {'class': 'course-header__department'})\n",
    "            course_info['facultyName'] = faculty_links[0].contents[0]\n",
    "          except:\n",
    "            course_info['facultyName'] = ''\n",
    "          \n",
    "          #Is it Fulltime?\n",
    "          try:\n",
    "            FullTime_links= soup.find_all('a', {'class':'concealLink' })\n",
    "            FullTime = False\n",
    "            for item in FullTime_links:\n",
    "              if item['href']== \"/masters-degrees/full-time/\":\n",
    "                FullTime = True\n",
    "                break\n",
    "            course_info['fullTime'] = FullTime\n",
    "          except:\n",
    "            course_info['fullTime'] = False\n",
    "\n",
    "          \n",
    "          #Description\n",
    "          try:\n",
    "            description = soup.find(\"div\", {\"class\": \"course-sections__description\"})\n",
    "            course_info['description'] = description.get_text(strip=True)\n",
    "          except:\n",
    "            course_info['description'] = ''\n",
    "\n",
    "          \n",
    "          #Start Date\n",
    "          try:\n",
    "            startDate = soup.find('span', {'class': 'key-info__start-date'})\n",
    "            course_info['startDate']= startDate.get_text(strip=True)\n",
    "          except:\n",
    "            course_info['startDate']= ''\n",
    "              \n",
    "          #Fees\n",
    "          try:\n",
    "            fees = soup.find('div', {'class': \"course-sections__fees\"})\n",
    "            course_info['fees']= fees.get_text(strip=True)\n",
    "          except:\n",
    "            course_info['fees']= ''\n",
    "              \n",
    "          #Modality\n",
    "          try:\n",
    "            modality = soup.find('span', {'class': \"key-info__qualification\"})\n",
    "            course_info['modality']= modality.get_text(strip=True)\n",
    "          except:\n",
    "            course_info['modality']= ''\n",
    "              \n",
    "          #Duration\n",
    "          try:\n",
    "            duration = soup.find('span', {'class': \"key-info__duration\"})\n",
    "            course_info['duration']= duration.get_text(strip=True)\n",
    "          except:\n",
    "            course_info['duration']= ''\n",
    "              \n",
    "          #City\n",
    "          try:\n",
    "            city = soup.find_all('a', {'class':'course-data__city'})\n",
    "            course_info['city']= city[0].get_text(strip=True)\n",
    "          except:\n",
    "            course_info['city']= ''\n",
    "              \n",
    "          #Country\n",
    "          try:\n",
    "            country= soup.find_all('a', {'class': 'course-data__country'})\n",
    "            course_info['country']= country[0].get_text(strip=True)\n",
    "          except:\n",
    "            course_info['country']= ''\n",
    "              \n",
    "          #Administration\n",
    "          try:\n",
    "            administration = soup.find_all('a', {'class': 'course-data__on-campus'})\n",
    "            course_info['administration']= administration[0].get_text(strip=True)\n",
    "          except:\n",
    "            course_info['administration']= ''\n",
    "              \n",
    "          #URL's\n",
    "          try:\n",
    "            url = soup.find(\"meta\", property=\"og:url\")\n",
    "            course_info['url']= url[\"content\"]\n",
    "            # print(url[\"content\"])\n",
    "          except:\n",
    "            url = \"\"          \n",
    "          \n",
    "          # Add the dictionary to the list\n",
    "          courses.append(course_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/laura/Data_Science/ADM/dataframe_HW3.csv\")\n",
    "df['description'] = df['description'].str.replace('About the course', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.0 Processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for general processing\n",
    "def processing(x):\n",
    "    #make sure that input is a string and case insensitive\n",
    "    words = nltk.word_tokenize(str(x).lower())\n",
    "    #stopwords, punctuation and nonnumerical\n",
    "    lst_stopwords = stopwords.words('english')\n",
    "    puncto = string.punctuation\n",
    "    text_preproc= [word for word in words if not word in lst_stopwords and word.isalpha() and word not in puncto]\n",
    "    #return the processed text as string\n",
    "    return ' '.join(text_preproc)\n",
    "\n",
    "#Delete the rows with no description\n",
    "df = df[df.description != '']\n",
    "\n",
    "#Apply the function to the description column and store result in desc_preproc\n",
    "df['desc_preproc'] = df['description'].apply(processing)\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmo_clean'] = df.desc_preproc.apply(lambda row: [stemmer.stem(word) for word in row.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1 Preprocessing the fees column\n",
    "\n",
    "First we want to extract the currency, we will do this by applying a function and storing the result in a new column called currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_currency(currency):\n",
    "    # Exception handling: Check if input is a string or if there is \n",
    "    if isinstance(currency, str):\n",
    "        # Find all currency symbols in the string\n",
    "        symbols = re.findall(r'[£$€¥¢]', currency)\n",
    "        # Return the first symbol found, or None if no symbols are found,also handels empty strings\n",
    "        return symbols[0] if symbols else None\n",
    "    else:\n",
    "        # Handle non-string inputs\n",
    "        return None\n",
    "#Applying the function to all rows in the dataframe and store the result in new column 'currency' \n",
    "df['currency'] = df['fees'].apply(extract_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a second step we extract the highest fee and store the result in a new column highest_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_highest_fee(fee):\n",
    "    if isinstance(fee, list):\n",
    "        # Exception handling: If the fee is a list, convert each item to a string and apply the function recursively\n",
    "        return max(extract_highest_fee(item) for item in fee) if fee else None\n",
    "    elif isinstance(fee, str):\n",
    "        # If the fee is a string, we just find all numbers and select the highest\n",
    "        numbers = re.findall(r'\\d+', fee)\n",
    "        numbers = [int(num) for num in numbers]\n",
    "        return max(numbers) if numbers else None\n",
    "    elif isinstance(fee, float):\n",
    "        # If the fee is a float, we don't have to do anything\n",
    "        return fee\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(fee)}\")\n",
    "\n",
    "df['highest_fee'] = df['fees'].apply(extract_highest_fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_currency = df['currency'].value_counts()\n",
    "print(which_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three different currencies, we decide to convert them to euros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eur(row):\n",
    "    #Write a currency mapping in order to make them convertible\n",
    "    currency_mapping = {\n",
    "    '$': 'USD',\n",
    "    '€': 'EUR',\n",
    "    '£': 'GBP',}\n",
    "    #Handling the case, that there was no currency found before\n",
    "    if pd.isnull(row['currency']):\n",
    "        return None\n",
    "    #Fees that we actually have to convert: Either Dollar or British Pound\n",
    "    elif row['currency'] != 'EUR':\n",
    "        cc = CurrencyConverter()\n",
    "        currency_code = currency_mapping.get(row['currency'])\n",
    "        if currency_code is not None:\n",
    "            currency_value = cc.convert(1, currency_code, 'EUR') * row['highest_fee']\n",
    "            return currency_value\n",
    "        #Exception handling: \n",
    "        else:\n",
    "            raise ValueError(f\"{row['currency']} is not a supported currency\")\n",
    "    #Currency is already Euros, we don't have to do anything        \n",
    "    else:\n",
    "        return row['highest_fee']\n",
    "\n",
    "#Apply the function to all rows in the dataframe and store it in the new column 'fees_in_eur'\n",
    "df['fees_in_eur'] = df.apply(convert_to_eur, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need in the column 'fees_in_eur'. We delete the old 'fees' column and rename the column 'fees_in_eur' to 'fees'. Further we delete the columns 'currency' and 'highest_fee' as we don't need them anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the old 'fees' column\n",
    "df.drop(['fees'], axis=1, inplace=True)\n",
    "\n",
    "#Rename the 'fees_in_eur' column\n",
    "df.rename(columns={'fees_in_eur': 'fees'}, inplace=True)\n",
    "\n",
    "#Delete the 'highest_fee' and the 'currency' column\n",
    "df.drop(['highest_fee', 'currency'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 100 rows of the dataframe and our results seem reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "#Put the elements of 'stemmo_clean' column into the vocabulary\n",
    "df.stemmo_clean.apply(lambda row: [vocabulary.add(word) for word in row])\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign an ID to each word in the vocabulary. We do this by creating a dictionnary called index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a dictionnary\n",
    "index = {}\n",
    "\n",
    "#We make a list out of the vocabulary in order to loop over it\n",
    "vocabulary = list(vocabulary) \n",
    "\n",
    "#Initialize first ID\n",
    "unique_id = 1\n",
    "\n",
    "#Loop over the vocabulary\n",
    "for word in vocabulary:\n",
    "    index[unique_id] = word\n",
    "    unique_id+=1\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the inverted index. It's a dictionnary called 'inverted_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "\n",
    "#Function to return True or False if the word is there\n",
    "def isthere(x, value):\n",
    "    if value in x:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Loop over the 'index dictionnary\n",
    "for key, value in index.items():\n",
    "    #Initialize a list to store all the row numbers where to word is present\n",
    "    listo = []\n",
    "    #Loop over the dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        #Function is called and adds every row where the word is present to the list\n",
    "        if isthere(row['stemmo_clean'], value):\n",
    "            listo.append(i)\n",
    "    #When we looped over all rows the list with the row numbers, the list is being assigned as value ro to correspoinding ID of the word\n",
    "    inverted_index[key] = listo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary so to not create it each time\n",
    "import pickle\n",
    "with open('inverted_index.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(inverted_index, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to load the dictionary\n",
    "with open('inverted_index.pkl', 'rb') as pickle_file:\n",
    "    inverted_index = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our first search engine. We do this by writing a fucntion that is called search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    #We split the input into single words and initialize a list\n",
    "    query_words = query.split()\n",
    "    document_lists = []\n",
    "    #Loop over the individual words\n",
    "    for word in query_words:\n",
    "        #As the words in our documents are preprocessed and stemmed we have to the same with our query\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        #We check if the word shows up in our documents and was assigned an ID\n",
    "        if stemmed_word in index.values():\n",
    "            #Loop over our 'index' dictionnary\n",
    "            for key, value in index.items():\n",
    "                if value == stemmed_word:\n",
    "                    #We add the the list of documents where the word showed up to the 'document_lists'\n",
    "                    document_lists.append(inverted_index[key])\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Unfortunately your search was not successful.\")\n",
    "    #If the search was successful we compute the intersection of the lists(as all the words must show up in the document)\n",
    "    if document_lists:\n",
    "        common_documents = set(document_lists[0]).intersection(*document_lists)\n",
    "        #Create the dataframe as requested in the task\n",
    "        new_df = df.loc[common_documents, ['courseName', 'universityName', 'description', 'url']].copy()\n",
    "        return new_df\n",
    "    #If there is no intersection between the lists of documents the search was not successfull\n",
    "    else:\n",
    "        print(f\"Unfortunately your search was not successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if our search engine works with the query used as an example in the task. The results we get seem reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('advanced knowledge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate our second inverted index, we call it 'inverted_index_due'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform our 'stemmo_clean' column to a list of strings. Each string is a document\n",
    "documents = [' '.join(words) for words in df['stemmo_clean'].tolist()]\n",
    "\n",
    "#Process data to be able to calculate the TFIDF\n",
    "matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names \n",
    "transformed_words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize the dictionnary\n",
    "inverted_index_due = {}\n",
    "\n",
    "# Loop over all the words\n",
    "for index, transformed_word in enumerate(transformed_words):\n",
    "    #Create the dictionnary key\n",
    "    inverted_index_due[transformed_word] = []\n",
    "    #Loop over all the rows\n",
    "    for row in range(len(df)):  \n",
    "        #Create tfidf score\n",
    "        tfidf_score = matrix[row, index]\n",
    "        #Documents with a tfidf of are left out\n",
    "        if tfidf_score > 0:\n",
    "            #Assign the tuple as value\n",
    "            inverted_index_due[transformed_word].append((df.index[index], tfidf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary so to not create it each time\n",
    "import pickle\n",
    "with open('inverted_index_due.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(inverted_index_due, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to load the dictionary\n",
    "with open('inverted_index_due.pkl', 'rb') as pickle_file:\n",
    "    inverted_index_due = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solve the about the course\n",
    "2. Proper preprocessing\n",
    "3. Clean dataframe: Make a string out of the list\n",
    "4. Preprocess inputqueries\n",
    "5. TSV file\n",
    "6. Save all the things that take long to compute in a file\n",
    "\n",
    "After we have created our inverted index, we create our second serach engine, we call it how_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_similar(query):\n",
    "    \n",
    "    #We preprocess our input quety\n",
    "    query= processing(query)\n",
    "    words = word_tokenize(query)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    stemmed_query = ' '.join(stemmed_words)\n",
    "    \n",
    "    # Make TFIDF representation of our query\n",
    "    query_tfidf = tfidf_vectorizer.transform([stemmed_query])\n",
    "    \n",
    "    # Calculate cosine similarities of the query with the documents\n",
    "    similarities = cosine_similarity(query_tfidf,matrix)\n",
    "    similarities = similarities.flatten()\n",
    "    \n",
    "    #We create the dataframe requested in the task\n",
    "    result_df= pd.DataFrame({\n",
    "    'courseName': df['courseName'],\n",
    "    'universityName': df['universityName'],\n",
    "    'description': df['description'],\n",
    "    'url': df['url'],\n",
    "    'similarity_score': similarities})\n",
    "    \n",
    "    #Sort, so that the most similar documents are shown first\n",
    "    result_df = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_similar('advanced knowledge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our search engine with some input and the results seem reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualizing the most relevant MSc degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "#  Google Maps Geocoding API using code for getting latitude and longitude \n",
    "def get_lat_long(api_key, city, country):\n",
    "    location = f\"{city}, {country}\"\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {'address': location, 'key': api_key}\n",
    "\n",
    "    # Retry until 3 failed requests\n",
    "    for attempt in range(3):\n",
    "        response = requests.get(base_url, params=params)\n",
    "        result = response.json()\n",
    "\n",
    "        if result['status'] == 'OK':\n",
    "            lat = result['results'][0]['geometry']['location']['lat']\n",
    "            lon = result['results'][0]['geometry']['location']['lng']\n",
    "            return lat, lon\n",
    "        else:\n",
    "            # Add a delay between retries\n",
    "            time.sleep(2)\n",
    "\n",
    "    # If still unsuccessful after retries, return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/lauralopez2001/ADM_HMW3/main/dataframe_HW3.csv')\n",
    "\n",
    "# Using google maps api key\n",
    "google_maps_api_key = 'AIzaSyBWtwu0b3vWFfm3i96fab33P7A3NxPD-fo'\n",
    "\n",
    "# Batch processing: specify the number of rows to process in each batch\n",
    "batch_size = 10\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i + batch_size].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    latitudes, longitudes = zip(\n",
    "        *batch_df.apply(lambda row: get_lat_long(google_maps_api_key, row['city'], row['country']), axis=1))\n",
    "    batch_df['Latitude'], batch_df['Longitude'] = latitudes, longitudes\n",
    "    df.loc[i:i + batch_size - 1, ['Latitude', 'Longitude']] = batch_df[['Latitude', 'Longitude']]\n",
    "\n",
    "# Create an interactive map using Plotly\n",
    "fig = px.scatter_geo(df, lat='Latitude', lon='Longitude', text='courseName',\n",
    "                     title='MSc Courses Worldwide',\n",
    "                     labels={'Latitude': 'Latitude', 'Longitude': 'Longitude'},\n",
    "                     hover_data={'Latitude': False, 'Longitude': False, 'courseName': True, 'city': True,\n",
    "                                 'country': True, 'fees': True})\n",
    "\n",
    "# Customize the map layout\n",
    "fig.update_geos(projection_type=\"natural earth\", showcoastlines=True, coastlinecolor=\"black\", showland=True,\n",
    "                landcolor=\"lightgray\")\n",
    "\n",
    "# Show the map\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Command Line Question\n",
    "Command line script is located in current directory with name __CommandLine.sh__<br />\n",
    "![Image command line question](terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Algorithm Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Implement a code to solve the above mentioned problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_for_target_sum(nums, target_sum):\n",
    "    n = len(nums)\n",
    "    dp = [[None for _ in range(target_sum + 1)] for _ in range(n + 1)]\n",
    "\n",
    "    # Base case: an empty subset can always add up to 0\n",
    "    for i in range(n):\n",
    "        if 0>=nums[i][0] and nums[i][1]>=0:\n",
    "            dp[i][0]=[0]\n",
    "        else:\n",
    "            dp[i][0]=[]\n",
    "    dp[n][0] = []\n",
    "\n",
    "    # Fill the dp table using bottom-up dynamic programming\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(target_sum + 1):\n",
    "            # Check if the subset sum can be formed without including the current element\n",
    "            dp[i][j] = dp[i - 1][j] if dp[i - 1][j] is not None else []\n",
    "\n",
    "            # Check if the subset sum can be formed by including the current element\n",
    "            lowerbound, upperbound = nums[i - 1]\n",
    "            for k in range(lowerbound, upperbound + 1):\n",
    "                if j >= k and dp[i - 1][j - k] is not None:\n",
    "                    dp[i][j] = dp[i - 1][j - k] + [k]\n",
    "                   \n",
    "\n",
    "    # If the target sum is not achievable, return an empty list\n",
    "    if not dp[n][target_sum]:\n",
    "        print(\"NO\")\n",
    "        return None\n",
    "\n",
    "    # Extract the actual elements by reconstructing the sequence\n",
    "    selected_elements = []\n",
    "    i, j = n, target_sum\n",
    "    \n",
    "    while i > 0 and j >= 0:\n",
    "        # try catch in case we can't reach our sum\n",
    "        try:\n",
    "            current_element = dp[i][j][-1]\n",
    "            selected_elements.append(current_element)\n",
    "            j -= current_element\n",
    "            i -= 1\n",
    "        except:\n",
    "            print(\"NO\")\n",
    "            return None\n",
    "    \n",
    "    final_str=\"\"\n",
    "    for x in selected_elements[::-1]:\n",
    "        final_str+=str(x)+\" \"\n",
    "    print(final_str)\n",
    "    print(\"YES\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # elaborate input\n",
    "    start_input=input()\n",
    "    start_input=start_input.split(\" \")\n",
    "    target_sum=int(start_input[1])\n",
    "    num_lines=int(start_input[0])\n",
    "    nums=[]\n",
    "    for i in range(num_lines):\n",
    "        start_line=input()\n",
    "        start_line=start_line.split()\n",
    "        tupler=(int(start_line[0]),int(start_line[1]))\n",
    "        nums.append(tupler)\n",
    "    # call to the function with nums variable:::: considered as list of tuples and target sum value we should reach\n",
    "    get_elements_for_target_sum(nums, target_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 What is the time complexity (the Big O notation) of your solution? Please provide a detailed explanation of how you calculated the time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity \"n=len(nums)\"- O(1), function len() of array in python takes up O(1) time <br />\n",
    "Time complexity to calculate dp table creation is O(target_sum*len(nums)) because of 2 loops <br />\n",
    "Time complexity for base case: O(len(nums)), because of the for loop <br />\n",
    "Time complexity for the filling: O(len(nums) * target_sum * (max(nums)-min(nums))), where max(nums) is a max number among entire  list of nums, min(nums) is a max number among entire list of nums <br />\n",
    "Time complexity for the last lines is O(len(selected_elements)) <br />\n",
    "Time complexity of algorithm=O(len(nums) * target_sum * (max(nums)-min(nums)))+O(len(nums))+O(target_sum*len(nums))+O(1)+O(len(selected_elements))=O(len(nums) * target_sum * (max(nums)-min(nums))) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Compare your answer to theirs. Do you believe this is correct? If the two differ, which one is right? (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer from chatgpt\n",
    "The time complexity of the given code is O(n * target_sum * (max_range - min_range)), where:\n",
    "\n",
    "n is the number of elements in the nums list.\n",
    "target_sum is the target sum for which we are trying to find a subset.\n",
    "max_range and min_range represent the upper and lower bounds for each range in the nums list.\n",
    "\n",
    "\n",
    "Actually there is no difference between two results, because n is a number of elements in list, and max_range and min_range are the same as max(nums) and min(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. What do you think of the optimality of your code? Do you believe it is optimal? Can you improve? Please elaborate on your response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is optimal because it is using dynamic programming, also considering the fact it is recursive problem, so from exponential cost we had been arrived to polinomial, which is good result "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.17 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
